name: Production Lead Pipeline

on:
  schedule:
    # Runs every Monday morning (0 1 * * 1) UTC for weekly lead refresh
    - cron: '0 1 * * 1'
  workflow_dispatch:
    # Allows manual triggering for testing

env:
  # CRITICAL: This variable holds the PostgreSQL connection string 
  # It MUST be set in your GitHub Secrets as "DATABASE_URL" 
  DATABASE_URL: ${{ secrets.DATABASE_URL }}

jobs:
  run_production_pipeline:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # We don't need GH_TOKEN anymore as we are not committing files.

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies (Requires Playwright/SQLAlchemy/psycopg2)
        run: pip install -r requirements.txt

      - name: Install Playwright Browsers (Required for real scraping)
        # This installs the necessary browser engine for the scraping script to work
        run: playwright install chromium --with-deps

      - name: Run Scraper
        # Creates data/raw/latest_raw_scrape.csv
        run: python scripts/scrape_sources.py
      
      - name: Run Cleaner and Verifier
        # Reads raw data, cleans it, and creates data/verified/verified_leads.csv
        run: python scripts/clean_verify.py

      - name: Database Update Success
        run: echo "âœ… Pipeline completed. Data sent directly to PostgreSQL database. Streamlit app will update automatically."
