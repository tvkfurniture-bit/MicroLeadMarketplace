name: Production Lead Pipeline

on:
  schedule:
    # Runs every Monday morning (0 1 * * 1) UTC for weekly lead refresh
    - cron: '0 1 * * 1'
  workflow_dispatch:
    # Allows manual triggering for testing

env:
  # CRITICAL: This variable holds the PostgreSQL connection string 
  # It MUST be set in your GitHub Secrets as "DATABASE_URL" 
  DATABASE_URL: ${{ secrets.DATABASE_URL }}

jobs:
  run_production_pipeline:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # We don't need GH_TOKEN anymore as we are not committing files.

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies (Requires Playwright/SQLAlchemy/psycopg2)
        run: pip install -r requirements.txt

      - name: Install Playwright Browsers (Required for real scraping)
        # This installs the necessary browser engine for the scraping script to work
        run: playwright install chromium --with-deps

      - name: Run Production Scraper and Verification
        # This single script now handles:
        # 1. Scraping data via Playwright.
        # 2. Cleaning/Verifying data.
        # 3. Inserting data directly into the PostgreSQL database using $DATABASE_URL.
        run: python scripts/real_scraper.py 
        # Note: We assume 'real_scraper.py' is the updated name for the combined script.

      - name: Database Update Success
        run: echo "âœ… Pipeline completed. Data sent directly to PostgreSQL database. Streamlit app will update automatically."
